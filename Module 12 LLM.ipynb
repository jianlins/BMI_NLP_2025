{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtUsmqxtNfmE"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jianlins/BMI_NLP_2025/blob/main/Module%2012%20LLM.ipynb)\n",
    "\n",
    "# Local LLM interactive demo\n",
    "\n",
    "This notebook uses Colab to demonstrate how to set up an LLM service, use langchain to work with the service with multiple LLM techniques.\n",
    "\n",
    "First we need to set up the LLM service. Make sure you use the GPU kernel, so that you won't be waiting too long for any responses. If you configured correctly, the following cell should display an Nvidia GPU information. For example:\n",
    "\n",
    "```\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
    "|-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
    "| N/A   57C    P0             29W /   70W |    1578MiB /  15360MiB |      0%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "                                                                                         \n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                              |\n",
    "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
    "|        ID   ID                                                               Usage      |\n",
    "|=========================================================================================|\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lfPiuLgmyRFO"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eE2u0Q7AyJoR"
   },
   "outputs": [],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6V3xFqPMzIc"
   },
   "outputs": [],
   "source": [
    "# install some linux dependencies, so that ollama can recognize GPU hardware. Tmux is a software to allow the service ran in the backend.\n",
    "!sudo apt install pciutils lshw tmux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnDX6wJAyMWl"
   },
   "outputs": [],
   "source": [
    "!lspci | grep -i nvidia\n",
    "!lshw -C display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HXyF8WG0X2n"
   },
   "outputs": [],
   "source": [
    "# start ollama server in the background\n",
    "!tmux new-session -d -s ollama 'ollama serve'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJU0W97W0p2N"
   },
   "outputs": [],
   "source": [
    "# download model in the background\n",
    "!tmux new-session -d -s qwen 'ollama pull qwen2.5-coder:1.5b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B13BpJ920rmg"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# test if model is loaded successfully, you might need to wait for around half a minute\n",
    "curl http://localhost:11434/api/generate -d '{\n",
    "  \"model\": \"qwen2.5-coder:1.5b\",\n",
    "  \"prompt\":\"Hello\"\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYGJEVFhNa09"
   },
   "source": [
    "## Install python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gj8Pz3SO1H5t"
   },
   "outputs": [],
   "source": [
    "!pip install langchain requests langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIrK6ikqRBKM"
   },
   "source": [
    "## Start a llm client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3tEVYVVa7NG"
   },
   "outputs": [],
   "source": [
    "!ollama stop deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yI5AAEzn5Vo3"
   },
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "llm  = OllamaLLM(model=\"qwen2.5-coder:1.5b\")\n",
    "output_parser= StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAgLrJ_fbMtE"
   },
   "source": [
    "# Zero shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TaQdMrrYRFBx"
   },
   "outputs": [],
   "source": [
    "# Define a zero-shot prompt for clinical entity extraction\n",
    "prompt_template = \"\"\"\n",
    "You are a clinical NLP assistant. Extract all clinical entities (diseases, medications, procedures) from the following text:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Convert the above prompt string into a chain, so that we can use it to process input text\n",
    "chain= PromptTemplate(template=prompt_template) | llm | output_parser\n",
    "\n",
    "# now we make a small input text and request the chain to process it and see what we can get in \"result\"\n",
    "clinical_text = \"Patient John Doe, 58, presents with chest pain and shortness of breath. Administered aspirin and nitroglycerin.\"\n",
    "result = chain.invoke(clinical_text)\n",
    "print(\"Extracted Entities:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vB0fGT9aRVTu"
   },
   "source": [
    "# Exercise 1:  \n",
    "\n",
    "Try to run the above cell multiple times, you may have noticed that each time the response is different, even though the main idea might be the same, right? How would to tweak the prompt to fix the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTR8KQtuRM5E"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ix4QJEBTSD9o"
   },
   "source": [
    "# A few shot learning\n",
    "Instead of tweaking the prompt request itself, we can actually show examples to LLM. This becomes a few shot learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFl58xs3SStL"
   },
   "outputs": [],
   "source": [
    "# Define a zero-shot prompt for clinical entity extraction\n",
    "prompt_template = \"\"\"\n",
    "You are a clinical NLP assistant. Extract all clinical entities (diseases, medications, procedures) from the following text.\n",
    "\n",
    "For example:\n",
    "Input:\n",
    "Patient John Doe, 58, presents with chest pain and shortness of breath. Administered aspirin and nitroglycerin.\n",
    "Output:\n",
    "1. Signs & Symptoms: chest pain, shortness of breath\n",
    "2. Medications: aspirin, nitroglycerin\n",
    "3. Procedures: None\n",
    "\n",
    "Input:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Convert the above prompt string into a chain, so that we can use it to process input text\n",
    "chain= PromptTemplate(template=prompt_template) | llm | output_parser\n",
    "\n",
    "# now we make a small input text and request the chain to process it and see what we can get in \"result\"\n",
    "clinical_text = \"Pt is a 58yo male, presents with chest pain and shortness of breath. Administered aspirin and nitroglycerin.\"\n",
    "result = chain.invoke(clinical_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxD14dGxVp3C"
   },
   "outputs": [],
   "source": [
    "# You might consider that the model just copy the answer. Let's try a different example\n",
    "prompt_template = \"\"\"\n",
    "You are a clinical NLP assistant. Extract all clinical entities (diseases, medications, procedures) from the following text.\n",
    "\n",
    "For example:\n",
    "Input:\n",
    "Patient John Doe, 58, presents with chest pain and shortness of breath. Administered aspirin and nitroglycerin.\n",
    "Output:\n",
    "1. Signs & Symptoms: chest pain, shortness of breath\n",
    "2. Medications: aspirin, nitroglycerin\n",
    "3. Procedures: None\n",
    "\n",
    "Input:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Convert the above prompt string into a chain, so that we can use it to process input text\n",
    "chain= PromptTemplate(template=prompt_template) | llm | output_parser\n",
    "\n",
    "# now we make a small input text and request the chain to process it and see what we can get in \"result\"\n",
    "clinical_text = \"A 67-year-old female presented to the emergency department with a sudden, severe headache, blurred vision, and confusion, accompanied by elevated blood pressure and an irregular pulse. On physical examination, she exhibited diaphoresis and nausea. A CT scan of the head revealed evidence of a hemorrhagic stroke. She was promptly managed with aggressive blood pressure control and underwent emergency neurosurgical intervention to evacuate the hematoma.\"\n",
    "result = chain.invoke(clinical_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CKiB1BzV2fy"
   },
   "source": [
    "# Let's try chain of thought (CoT)\n",
    "\n",
    "Chain of Thought prompting for clinical entity extraction\n",
    "\n",
    "Chain-of-thought (CoT) prompting is an approach that encourages language models to break down complex problems into intermediate reasoning steps before arriving at a final answer. By guiding the model to “think aloud,” CoT helps reveal the internal decision-making process, making its outputs more transparent and often more accurate, especially for tasks that involve multi-step logic or complex reasoning. This method not only improves problem-solving performance but also provides valuable insights into how the model processes information—a feature that can be crucial in sensitive domains like clinical decision support.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nwigNilZY6At"
   },
   "outputs": [],
   "source": [
    "# let's try a different task using zero shot learning, and see how it does:\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a clinical NLP assistant, given the provided clinical text, try to list the most likely diagnoses.\n",
    "\n",
    "Input:\n",
    "{text}\n",
    "\"\"\"\n",
    "chain= PromptTemplate(template=prompt_template) | llm | output_parser\n",
    "clinical_text = \"A 67-year-old female presented to the emergency department with a sudden, severe headache, blurred vision, and confusion, accompanied by elevated blood pressure and an irregular pulse. On physical examination, she exhibited diaphoresis and nausea. A CT scan of the head revealed evidence of a hemorrhagic stroke. She was promptly managed with aggressive blood pressure control and underwent emergency neurosurgical intervention to evacuate the hematoma.\"\n",
    "result = chain.invoke(clinical_text)\n",
    "print(\"Extracted Entities:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9LuyJVmWp_u"
   },
   "outputs": [],
   "source": [
    "# Define a zero-shot prompt for clinical entity extraction\n",
    "prompt_template = \"\"\"\n",
    "You are a clinical NLP assistant. Analyze the following clinical text by breaking down your reasoning into steps.\n",
    "\n",
    "Step 1: Identify and list key signs, symptoms, and clinical details.\n",
    "Step 2: Note any tests, procedures, or treatments mentioned.\n",
    "Step 3: Based on the collected information, propose possible diagnoses.\n",
    "Step 4: Explain briefly why these diagnoses are the most likely.\n",
    "Step 5: Verify and summarize the proposed diagnoses using a json format string: {{\"diagnoses\": [\"diagnosis1\", \"diagnosis2\", ...]}}\n",
    "\n",
    "Input:\n",
    "{text}\n",
    "\"\"\"\n",
    "# Convert the above prompt string into a chain, so that we can use it to process input text\n",
    "chain= PromptTemplate(template=prompt_template) | llm | output_parser\n",
    "\n",
    "# now we make a small input text and request the chain to process it and see what we can get in \"result\"\n",
    "clinical_text = \"A 67-year-old female presented to the emergency department with a sudden, severe headache, blurred vision, and confusion, accompanied by elevated blood pressure and an irregular pulse. On physical examination, she exhibited diaphoresis and nausea. A CT scan of the head revealed evidence of a hemorrhagic stroke. She was promptly managed with aggressive blood pressure control and underwent emergency neurosurgical intervention to evacuate the hematoma.\"\n",
    "result = chain.invoke(clinical_text)\n",
    "print(\"Extracted Entities:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXoIZdnJcSCm"
   },
   "source": [
    "# Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) combines information retrieval techniques with large language models to enhance response quality. In RAG, relevant documents or passages are first retrieved from a knowledge base, then the language model uses that context to generate more informed and accurate responses. This hybrid approach helps ensure that the generated text is both contextually rich and up-to-date.\n",
    "\n",
    "I prepared the following guideline by asking chatgpt: \"Summarize the page below to a guideline for LLM use: \" and copy the content of the page: https://www.mayoclinic.org/diseases-conditions/stroke/diagnosis-treatment/drc-20350119.\n",
    "\n",
    "\n",
    "This is just a demo of RAG, where how to get the augmented content is not included here. But you can see how the augumentation can help with the LLM's response. You can read more about RAG if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLpz41zG59GC"
   },
   "outputs": [],
   "source": [
    "\n",
    "guideline='''# Guideline for Recognition of Stroke Symptoms (FAST):\n",
    "\n",
    "F: Facial weakness\n",
    "\n",
    "A: Arm weakness\n",
    "\n",
    "S: Speech slurring\n",
    "\n",
    "T: Time – immediate action is critical\n",
    "Emphasize that if these symptoms are observed—even if transient—they warrant urgent emergency evaluation.\n",
    "\n",
    "Diagnostic Evaluation:\n",
    "\n",
    "Initial Assessment: Highlight the importance of a physical and neurological exam along with vital sign monitoring.\n",
    "\n",
    "Imaging and Tests: Include the use of CT scans, MRI, carotid ultrasound, cerebral angiograms, and echocardiograms to differentiate stroke types and determine the extent of brain injury.\n",
    "\n",
    "Stroke Types and Treatments:\n",
    "\n",
    "Ischemic Stroke: Explain that blood clots can block blood flow, and timely treatment (e.g., IV TPA within the therapeutic window and endovascular procedures) is essential to restore circulation and minimize brain damage.\n",
    "\n",
    "Hemorrhagic Stroke: Describe how a ruptured aneurysm or other vascular abnormality causes bleeding, requiring rapid measures to control bleeding, relieve brain pressure, and possibly surgical intervention (clipping, coiling, or radiosurgery).\n",
    "\n",
    "Prevention and Risk Management:\n",
    "\n",
    "Risk Factors: Distinguish between non-modifiable (age, gender, family history) and modifiable (high blood pressure, cholesterol, smoking, diabetes, etc.) risk factors.\n",
    "\n",
    "Prevention Strategies: Stress the importance of managing modifiable risk factors through lifestyle changes and medications, which are more effective than post-stroke treatments.\n",
    "\n",
    "Rehabilitation and Recovery:\n",
    "\n",
    "Multidisciplinary Approach: Outline that recovery involves a team of healthcare professionals (neurologists, rehabilitation physicians, therapists, etc.) who guide the patient through physical, occupational, and speech therapies.\n",
    "\n",
    "Gradual Process: Communicate that stroke recovery is gradual, and continuous therapy over months can lead to significant improvements.\n",
    "\n",
    "Patient Education and Urgency:\n",
    "\n",
    "Emergency Action: Reinforce that stroke symptoms require immediate medical attention to improve outcomes.\n",
    "\n",
    "Ongoing Support: Provide reassurance regarding the availability of support, both medical and emotional, throughout the recovery process.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcLBi-UIcU37"
   },
   "outputs": [],
   "source": [
    "# Define a zero-shot prompt for clinical entity extraction\n",
    "prompt_template = guideline+\"\"\"\n",
    "\n",
    "You are a clinical NLP assistant. Analyze the following clinical text using the guideline above and break down your reasoning into steps.\n",
    "\n",
    "Step 1: Identify and list key signs, symptoms, and clinical details.\n",
    "Step 2: Note any tests, procedures, or treatments mentioned.\n",
    "Step 3: Based on the collected information, propose possible diagnoses.\n",
    "Step 4: Explain briefly why these diagnoses are the most likely.\n",
    "Step 5: Verify and summarize the proposed diagnoses using a json format string: {{\"diagnoses\": [\"diagnosis1\", \"diagnosis2\", ...]}}\n",
    "\n",
    "Input:\n",
    "{text}\n",
    "\"\"\"\n",
    "# Convert the above prompt string into a chain, so that we can use it to process input text\n",
    "chain= PromptTemplate(template=prompt_template) | llm | output_parser\n",
    "\n",
    "# now we make a small input text and request the chain to process it and see what we can get in \"result\"\n",
    "clinical_text = \"A 67-year-old female presented to the emergency department with a sudden, severe headache, blurred vision, and confusion, accompanied by elevated blood pressure and an irregular pulse. On physical examination, she exhibited diaphoresis and nausea. A CT scan of the head revealed evidence of a hemorrhagic stroke. She was promptly managed with aggressive blood pressure control and underwent emergency neurosurgical intervention to evacuate the hematoma.\"\n",
    "result = chain.invoke(clinical_text)\n",
    "print(\"Extracted Entities:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rb22L862dfR4"
   },
   "source": [
    "# Exercise 2:\n",
    "\n",
    "Think about your own project, can you try one or two examples of your data and design a prompt to work out a solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9s_Z1FFci6W"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMtbE1/lZt7kbNIgOmpDCIb",
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
