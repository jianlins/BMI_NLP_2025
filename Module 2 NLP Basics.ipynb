{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIOpd4RUuALB"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jianlins/BMI_NLP_2025/blob/main/Module%202%20NLP%20Basics.ipynb)\n",
    "# Welcome to the \"Colab/SpaCy\" demo for Module 2.\n",
    "\n",
    "Google offers this browser-based Python development environment, called \"Colab\", that replicates most of the functionality of a Jupyter notebook. We plan to use it in class for future assignments. So we wanted you to walk through a gentle introduction here.\n",
    "\n",
    "You can see right away that Colab looks a lot like the notebooks we have used in class. Just as in a notebook, you use the SHIFT-RETURN keys to run the code in a cell OR you can click the \"run arrow\" on the left of code cells.\n",
    "\n",
    "Our goal with this Colab notebook is simply to show you Colab itself, as well as to introduce you to a few spaCy functions. IF YOU HAVE NOT READ the Mod2/Week2 Canvas text yet, STOP, READ IT, and THEN RETURN HERE. Be thinking about spaCy's output compared to similar functions from NLTK or coreNLP that you have already seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f7l_poGGeuH"
   },
   "source": [
    "# Set up the environment\n",
    "In colab, a set of commonly used python packages have already been installed by default. But spaCy is not one of them. So we need to install it ourselves. _Note that the next cell may take a minute or two to load._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nkP896c-3Fbz"
   },
   "outputs": [],
   "source": [
    "!pip install medspacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3P1AWc13C75"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from pathlib import Path\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kjbeb7Ee3C78"
   },
   "source": [
    "# Overview\n",
    "In this notebook, we will use spaCy's visualization tool to demonstrate the basic NLP concepts that we covered in the course text. We will cut short a discussion of what these NLP basics are (that's what we did in the Canvas course text). You can use the \"Table of contents\" on the left to navigate through different NLP basics.\n",
    "\n",
    "First, let's get a sample text into the Python variable \"text\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LnUZ8Ok-3C7-"
   },
   "outputs": [],
   "source": [
    "text='''HX: This 64 y/o RHM had had difficulty remembering names, phone numbers and events for 12 months prior to presentation, on 2/28/95. This had been called to his attention by the clerical staff at his parish--he was a Catholic priest. He had had no professional or social faux pas or mishaps due to his memory. He could not tell whether his problem was becoming worse, so he brought himself to the Neurology clinic on his own referral.'''\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyhgK3ghQTrc"
   },
   "source": [
    "spaCy does not have a default visualization for tokens or sentences. Although visualization is not necessary for the code to run, it can be useful for humans trying to learn about NLP basics. So we will play a few tricks here to visualize tokens, sentences, and POS. That way you can get a tangible sense of what these basic functions are really doing \"under the hood.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOplJO0CQRzE"
   },
   "outputs": [],
   "source": [
    "def vis_tokens(doc, color='#c4e6e2'):\n",
    "  # convert tokens into entities for visualization\n",
    "  doc.ents=[doc.char_span(t.idx, t.idx+len(t), label=\"Token\") for t in doc]\n",
    "  displacy.render(doc, jupyter=True, style=\"ent\", options={'colors':{'Token':color}})\n",
    "\n",
    "def vis_sentences(doc, color='#89C4F4'):\n",
    "  # convert sentences into entities for visualization\n",
    "  doc.ents=[doc.char_span(s.start_char, s.end_char, label=\"Sentence\") for s in doc.sents]\n",
    "  displacy.render(doc, jupyter=True, style=\"ent\", options={'colors':{'Sentence':color}})\n",
    "\n",
    "def vis_pos(doc, colors={}):\n",
    "  colors={'ADJ':'#ffccd5', 'ADP':'#dee2e6','ADV':'#e2ece9','AUX':'#dfe7fd',\n",
    "        'CCONJ':'#e9ecef','CCONJ':'#adb5bd','DET':'#edede9','INTJ':'#fad2e1',\n",
    "        'NOUN':'#fff1e6','NUM':'#fde2e4','PART':'#f0efeb','PRON':'#eddcd2','PROPN':'#f0efeb','PUNCT':'#eae4e9',\n",
    "        'SCONJ':'#8e9aaf','SYM':'#bee1e6','VERB':'#cddafd','X':'#edf2fb'}\n",
    "  # spaCy has another style of visualization to display POS tags, but here, before we throw in additional jargon, we use the same trick to render these tags\n",
    "  doc.ents=[doc.char_span(t.idx, t.idx+len(t), label=t.pos) for t in doc]\n",
    "  displacy.render(doc, jupyter=True, style=\"ent\",options={'colors':colors})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tnQQ9iBTNbg"
   },
   "source": [
    "## Processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RFjsub57TFji"
   },
   "outputs": [],
   "source": [
    "# load a spaCy model (similar to the concept of \"pipeline\" that we will cover in the next module). SpaCy\n",
    "# wraps everything together into \"nlp\" so we don't need to execute each task individually\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtD70aQyToF_"
   },
   "outputs": [],
   "source": [
    "# process the text--- one line code, simple, isn't it?\n",
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrAU3IUE3C8A"
   },
   "source": [
    "# Tokenization\n",
    "As we mentioned above, all the NLP basic tasks have been executed in the single line of code above. We don't need to run tokenization separately here. But do want to *see* the tokens, so we run the vis_tokens() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nv9IhVf_TzVY"
   },
   "outputs": [],
   "source": [
    "vis_tokens(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bN1AlJ13C8P"
   },
   "source": [
    "# Sentence segmentation\n",
    "Same here. We just show the sentences that have been split. Note that spaCy decided \"HX:\" was a self-contained sentence. Do you agree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufLY-OGBUVhp"
   },
   "outputs": [],
   "source": [
    "vis_sentences(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VH5LFyYLNFm_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ei5saxY0V76z"
   },
   "source": [
    "# POS tagging\n",
    "When we started this Colab we loaded a general \"language model\" called \"en_core_web_sm.\" That model is configured using a smaller amount of parameters. The text we defined here is clinical texts. So clinical idioms like \"HX\" (short for \"history of present illness\") can confuse spaCy if it is looking for general English. The visualization below shows why it makes sense for humans to review visualized output. We can see that spaCy thought \"HX:\" was a proper noun (see POS schema below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SW0y28qV9Tj"
   },
   "outputs": [],
   "source": [
    "vis_pos(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ivs0omEAckkK"
   },
   "source": [
    "## POS schema used in spaCy\n",
    "| POS |\tDESCRIPTION  |\tEXAMPLES |\n",
    "| --- | --------- |  ----------------------------------------- |\n",
    "| ADJ | adjective | *big, old, green, incomprehensible, first* |\n",
    "| ADP | adposition | *in, to, during* |\n",
    "| ADV | adverb | *very, tomorrow, down, where, there* |\n",
    "| AUX | auxiliary | *is, has (done), will (do), should (do)* |\n",
    "| CONJ | conjunction | *and, or, but* |\n",
    "| CCONJ | coordinating conjunction | *and, or, but* |\n",
    "| DET | determiner | *a, an, the* |\n",
    "| INTJ | interjection | *psst, ouch, bravo, hello* |\n",
    "| NOUN | noun | *girl, cat, tree, air, beauty* |\n",
    "| NUM | numeral | *1, 2017, one, seventy-seven, IV, MMXIV* |\n",
    "| PART | particle | *‚Äôs, not,* |\n",
    "| PRON | pronoun | *I, you, he, she, myself, themselves, somebody* |\n",
    "| PROPN | proper noun | *Mary, John, London, NATO, HBO* |\n",
    "| PUNCT | punctuation | *., (, ), ?* |\n",
    "| SCONJ | subordinating conjunction | *if, while, that* |\n",
    "| SYM | symbol | *$, %, ¬ß, ¬©, +, ‚àí, √ó, √∑, =, :), üòù* |\n",
    "| VERB | verb | *run, runs, running, eat, ate, eating* |\n",
    "| X | other | *sfpksdpsxmsa* |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SaiVC_Jfs5c"
   },
   "source": [
    "# Dependency parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wo3APehtcjKc"
   },
   "outputs": [],
   "source": [
    "  displacy.render(list(doc.sents)[2], jupyter=True, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd8DuWqRf96N"
   },
   "source": [
    "# Named entity recognition\n",
    "Since our code above messed with the entities in the \"doc\", we redo the \"doc\" processing before we display the **real** named entities spaCy is looking for. The model en_core_web_sm is looking for named entities like `DATEs (DATE), Nationality or religion (NORP), and facilities (like named bridges, buidings, etc.)`. Hmm. Is the number 64 really a date here? In spaCy's view it is. To spaCy a date is \"an absolute or relative date or time period.\" If spaCy were looking at the phrase \"64 y/o\" then `DATE` makes sense. And 'Catholic' is definitely a religion. But why it concluded \"2/28/95\" was a cardinal number is not clear.\n",
    "\n",
    "The lesson? NLP is never perfect. So we 'ALWAYS' have to evlautae how well our NLP systems are doing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7p3k_mIgAPx"
   },
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3TAL5O1zgBxK"
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, jupyter=True, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwdw9A41Z4U7"
   },
   "source": [
    "The following table explains these labels\n",
    "\n",
    "| LABEL | DESCRIPTION   |\n",
    "| --- | ------------------------------------------------------- |\n",
    "| DATE | Absolute or relative dates or periods |\n",
    "| PERSON | People, including fictional |\n",
    "| GPE | Countries, cities, states |\n",
    "| LOC | Non-GPE locations, mountain ranges, bodies of water |\n",
    "| MONEY | Monetary values, including unit |\n",
    "| TIME | Times smaller than a day |\n",
    "| PRODUCT | Objects, vehicles, foods, etc. (not services) |\n",
    "| CARDINAL | Numerals that do not fall under another type |\n",
    "| ORDINAL | \"first\", \"second\", etc. |\n",
    "| QUANTITY | Measurements, as of weight or distance |\n",
    "| EVENT | Named hurricanes, battles, wars, sports events, etc. |\n",
    "| FAC | Buildings, airports, highways, bridges, etc. |\n",
    "| LANGUAGE | Any named language |\n",
    "| LAW | Named documents made into laws. |\n",
    "| NORP | Nationalities or religious or political groups |\n",
    "| PERCENT | Percentage, including \"%\" |\n",
    "| WORK_OF_ART | Titles of books, songs, etc. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dadwxZ6gVH9J"
   },
   "source": [
    "## Exercise:\n",
    "\n",
    "Now we understand that 'en_core_web_sm' may not suffice for basic NER tasks, even though it serves as a good starting point for experimentation. If you're curious about the performance of larger models, consider trying the following:\n",
    "*   en_core_web_md\n",
    "*   en_core_web_lg\n",
    "\n",
    "\n",
    "en_core_web_lg\n",
    "\n",
    "Additionally, spaCy offers a customized transformer model specifically tailored for NER tasks. We'll delve deeper into transformers later in the course. For now, let's just think of it as a much larger model. You will need to install spacy-transformers and download the model before you can load it.\n",
    "*   en_core_web_trf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cmj_ez9dVG7R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Xg6OhYt7KKI"
   },
   "source": [
    "# QuickUMLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmvVmIuIXbIq"
   },
   "source": [
    "In the Module pages we mentioned QuickUMLS. Although its full functioning requires a full UMLS installation (long time to load and very space  hungry), we can demonstrate it using a made-up \"UMLS\" with 3 CUIs hardcoded as below.\n",
    "\n",
    "QuickUMLS needs two files: *MRCONSO.RRF* and *MRSTY.RRF*. We follow the same format of these two files, and create a customized dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IA97Dp_i7Y0Y"
   },
   "outputs": [],
   "source": [
    "mrconso_text=r'''C0000001|ENG|S|L0000002|VO|S0000895|N|A0000030||||BI|PT|BI00001|difficulty remembering|2|N|256|\n",
    "C0000001|ENG|S|L0000002|VO|S0000895|N|A0000031||||BI|PT|BI00001|memory lost|2|N|256|\n",
    "C0000002|ENG|S|L0000003|VO|S0000005|N|A0000041||||BI|PT|BI00002|neurology clinic|2|N|256|'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvXYNiY988oS"
   },
   "outputs": [],
   "source": [
    "mrsty_text=r'''C0000001|T191|||||\n",
    "C0000002|T191|||||'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AKoomnin9cwO"
   },
   "outputs": [],
   "source": [
    "Path('MRCONSO.RRF').write_text(mrconso_text)\n",
    "Path('MRSTY.RRF').write_text(mrsty_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ux9bVMunUzPC"
   },
   "outputs": [],
   "source": [
    "# the location of the default quickumls (medspacy's version) data\n",
    "default_medspacy_quickumls_dir='/usr/local/lib/python3.10/dist-packages/resources/quickumls/QuickUMLS_SAMPLE_lowercase_POSIX_unqlite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5nsFJ2cVZ9f"
   },
   "outputs": [],
   "source": [
    "# clean up the folder\n",
    "if Path(default_medspacy_quickumls_dir).exists:\n",
    "    import shutil\n",
    "    shutil.rmtree(Path(default_medspacy_quickumls_dir))\n",
    "Path(default_medspacy_quickumls_dir).mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kut0RJZ-nQC"
   },
   "outputs": [],
   "source": [
    "# install our customized \"UMLS\"\n",
    "!python -m quickumls.install  ./ /usr/local/lib/python3.10/dist-packages/resources/quickumls/QuickUMLS_SAMPLE_lowercase_POSIX_unqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvBbcTn-BZwm"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import medspacy\n",
    "from quickumls.spacy_component import SpacyQuickUMLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqkyJYGJCrTF"
   },
   "outputs": [],
   "source": [
    "# start from a default spacy language model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijr-lJdzY0EA"
   },
   "outputs": [],
   "source": [
    "nlp.add_pipe('medspacy_quickumls', config={\"quickumls_fp\": default_medspacy_quickumls_dir})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXBnVY_ZNIbI"
   },
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0vYwHnr3NKG2"
   },
   "outputs": [],
   "source": [
    "# Now you can see two additional named entities have been annotated and labeled with CUIs.\n",
    "displacy.render(doc, jupyter=True, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0gcjbzobvYC"
   },
   "source": [
    "#So that was a super simple illustration of QuickUMLS.\n",
    "\n",
    "We will not be using QuickUMLS more in the course, but we will refer often to the UMLS itself. This last exercise is meant to reinforce the idea that the UMLS has many concepts and has many **more** terms that map to those concepts. Tools like medspacy and CLAMP can help identify these concepts automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flHnnHspcLoe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M14QlOEScbfr"
   },
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1dLOehbJm90_x7WpGdqXB0YS77e_4LDp_",
     "timestamp": 1704946683510
    },
    {
     "file_id": "19DHk0le1IOVOdJBiGv3TkyVJut-JmWv2",
     "timestamp": 1643235439035
    },
    {
     "file_id": "https://github.com/medspacy/medspacy/blob/master/notebooks/02-Tokenizing-and-Sentence-Splitting.ipynb",
     "timestamp": 1642186171554
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
